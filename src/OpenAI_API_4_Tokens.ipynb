{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMScUcgxXPbSQ2ZNePjZjus",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alarcon7a/openai-api-tutorial/blob/main/src/OpenAI_API_4_Tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalar librerias"
      ],
      "metadata": {
        "id": "_BRXYlhZAdK_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExSJeHH666kP"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade tiktoken==0.8.0 -q\n",
        "%pip install --upgrade openai==1.58.1 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "OJRe8DJ48Ufw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding y Decoding"
      ],
      "metadata": {
        "id": "OtDxx0_lAh-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Encoding name           | OpenAI models                                                                 |\n",
        "|-------------------------|-------------------------------------------------------------------------------|\n",
        "| `o200k_base`            | `gpt-4o`, `gpt-4o-mini`                                                      |\n",
        "| `cl100k_base`           | `gpt-4-turbo`, `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large` |\n",
        "| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`                         |\n",
        "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                                                  |\n"
      ],
      "metadata": {
        "id": "tDNBlpLIBeHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Importamos la librería\n",
        "import tiktoken\n",
        "\n",
        "# 2. Cargamos la codificación\n",
        "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
        "\n",
        "# 3. Codificamos texto a tokens\n",
        "text = \"¡Hola, mundo! Esto es una prueba.\"\n",
        "tokens = encoding.encode(text)\n",
        "print(f\"Texto: {text}\")\n",
        "print(f\"Tokens (enteros): {tokens}\")\n",
        "\n",
        "# 4. Decodificamos tokens a texto\n",
        "decoded_text = encoding.decode(tokens)\n",
        "print(f\"Texto decodificado: {decoded_text}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_35_5j47QQ7",
        "outputId": "d6db15ef-1e58-4a73-9c25-71e1817796f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: ¡Hola, mundo! Esto es una prueba.\n",
            "Tokens (enteros): [20407, 49864, 11, 10225, 0, 43584, 878, 1969, 49548, 13]\n",
            "Texto decodificado: ¡Hola, mundo! Esto es una prueba.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para obtener los bytes de cada token\n",
        "token_bytes = [encoding.decode_single_token_bytes(token) for token in tokens]\n",
        "print(f\"Texto: {text}\")\n",
        "print(\"\\nTokens (bytes):\", token_bytes)\n",
        "print(\"\\nTotal de tokens\", len(token_bytes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbMMjJJo9VzE",
        "outputId": "f2a3f76c-3a6c-4d67-c4f4-fa93c0182b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: ¡Hola, mundo! Esto es una prueba.\n",
            "\n",
            "Tokens (bytes): [b'\\xc2\\xa1', b'Hola', b',', b' mundo', b'!', b' Esto', b' es', b' una', b' prueba', b'.']\n",
            "\n",
            "Total de tokens 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Contamos tokens de un string\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens = num_tokens_from_string(\"Suscribete al canal\", \"o200k_base\")\n",
        "print(f\"Numero de tokens: {num_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGtQSl1w8u3o",
        "outputId": "24038662-ccd5-44d7-c9fb-99458d393e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de tokens: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculadora en python"
      ],
      "metadata": {
        "id": "02OV8wHm-C-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0125\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "        \"gpt-4o-mini-2024-07-18\",\n",
        "        \"gpt-4o-2024-08-06\"\n",
        "        }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0125\")\n",
        "    elif \"gpt-4o-mini\" in model:\n",
        "        print(\"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\")\n",
        "    elif \"gpt-4o\" in model:\n",
        "        print(\"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4o-2024-08-06\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens\n",
        "\n",
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Tell me a joke.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "num_tokens = num_tokens_from_messages(example_messages, \"gpt-4o-mini\")\n",
        "print(f\"Number of tokens in messages: {num_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIhbapcv-Fn_",
        "outputId": "79e2d41b-b045-4a56-f34f-549b834f262f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
            "Number of tokens in messages: 22\n"
          ]
        }
      ]
    }
  ]
}
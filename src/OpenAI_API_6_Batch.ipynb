{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz5kIwJdldskcqrb0UUusg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alarcon7a/openai-api-tutorial/blob/main/src/OpenAI_API_6_Batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai"
      ],
      "metadata": {
        "id": "-aROO1-Pv2j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y179E7vv05F"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI, AzureOpenAI\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "api_key_openai = userdata.get(\"OPENAI_API_KEY\")\n",
        "client_openai = OpenAI(api_key=api_key_openai)"
      ],
      "metadata": {
        "id": "Xs77rXWnwE_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Creando nuestro archivo jsonl"
      ],
      "metadata": {
        "id": "6UCR4ZSPr8kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def create_batch_file(requests: List[Dict[str, str]], output_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Crea un archivo JSONL para el batch con las solicitudes especificadas.\n",
        "\n",
        "    Args:\n",
        "        requests: Lista de diccionarios con id y content\n",
        "        output_path: Ruta donde se guardará el archivo JSONL\n",
        "\n",
        "    Returns:\n",
        "        Ruta del archivo creado\n",
        "    \"\"\"\n",
        "    print('Creando archivo de batch...')\n",
        "\n",
        "    # Crear contenido JSONL\n",
        "    jsonl_lines = []\n",
        "    for req in requests:\n",
        "        batch_request = {\n",
        "            \"custom_id\": req[\"id\"],\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": \"gpt-4o\",  # Asegúrate de usar un modelo compatible con Batch API\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"Responde como un pirata\"},\n",
        "                    {\"role\": \"user\", \"content\": req[\"content\"]}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "        jsonl_lines.append(json.dumps(batch_request))\n",
        "\n",
        "    # Escribir archivo\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(jsonl_lines))\n",
        "\n",
        "    print(f'Archivo batch creado en: {output_path}')\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "sc7kOcC-xGct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "requests = [\n",
        "      {\"id\": \"req1\", \"content\": \"¿Qué es la inteligencia artificial?\"},\n",
        "      {\"id\": \"req2\", \"content\": \"Explica la diferencia entre machine learning y deep learning\"},\n",
        "      {\"id\": \"req3\", \"content\": \"¿Cuáles son las aplicaciones del procesamiento de lenguaje natural?\"},\n",
        "      {\"id\": \"req4\", \"content\": \"Describe el funcionamiento básico de ChatGPT\"},\n",
        "      {\"id\": \"req5\", \"content\": \"¿Qué es un modelo de embeddings y para qué sirve?\"}\n",
        "  ]\n",
        "\n",
        "  # Crear archivo de batch\n",
        "batch_file_path = create_batch_file(requests, '/content/openai_batch.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMBrRmnZxJrF",
        "outputId": "ee6b3739-0d37-47ee-cd61-f48a5bd6e4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando archivo de batch...\n",
            "Archivo batch creado en: /content/openai_batch.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "abQ_1KCQt1ZS",
        "outputId": "976b60c6-823b-430b-a4bd-d9cca777cd6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/openai_batch.jsonl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Subir el archivo de entrada"
      ],
      "metadata": {
        "id": "rwgXslZksQhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_file(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Sube el archivo JSONL a OpenAI.\n",
        "\n",
        "    Args:\n",
        "        file_path: Ruta al archivo JSONL\n",
        "\n",
        "    Returns:\n",
        "        ID del archivo subido\n",
        "    \"\"\"\n",
        "    print('Subiendo archivo a OpenAI...')\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = client_openai.files.create(\n",
        "            file=f,\n",
        "            purpose=\"batch\",\n",
        "        )\n",
        "\n",
        "    file_id = response.id\n",
        "    print(f'Archivo subido con ID: {file_id}')\n",
        "    return file_id"
      ],
      "metadata": {
        "id": "P4phj-GCycCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = upload_file('/content/openai_batch.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX8D3TRXxkEv",
        "outputId": "960db9f4-f477-48bb-a3f9-2f10fe044a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subiendo archivo a OpenAI...\n",
            "Archivo subido con ID: file-8BD39uTxNdwEURFFgnGjGF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1CBSsrYOuNaf",
        "outputId": "442d35ef-ba79-4fd8-cd6e-4299bee60d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'file-8BD39uTxNdwEURFFgnGjGF'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Crear nuestro lote (Batch)"
      ],
      "metadata": {
        "id": "V1szEHzasaU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch(input_file_id: str):\n",
        "    \"\"\"\n",
        "    Crea un batch en OpenAI con el archivo subido.\n",
        "\n",
        "    Args:\n",
        "        input_file_id: ID del archivo subido a OpenAI\n",
        "\n",
        "    Returns:\n",
        "        Objeto batch creado\n",
        "    \"\"\"\n",
        "    print('Creando batch...')\n",
        "\n",
        "    batch = client_openai.batches.create(\n",
        "        input_file_id=input_file_id,\n",
        "        endpoint=\"/v1/chat/completions\",\n",
        "        completion_window=\"24h\",\n",
        "        metadata={\n",
        "            \"description\": \"Demo batch para video de YouTube\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f'Batch creado con ID: {batch.id}')\n",
        "    return batch"
      ],
      "metadata": {
        "id": "6BpRcfgJ2pBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_batch(file_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ibXiJtP3EhC",
        "outputId": "4695a1cf-079f-4d33-fb78-0c901fc02e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando batch...\n",
            "Batch creado con ID: batch_682406de18708190bed4e66a925900b7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Batch(id='batch_682406de18708190bed4e66a925900b7', completion_window='24h', created_at=1747191518, endpoint='/v1/chat/completions', input_file_id='file-8BD39uTxNdwEURFFgnGjGF', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747277918, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Demo batch para video de YouTube'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Verificar el estado del lote\n"
      ],
      "metadata": {
        "id": "gkjZQgdTselv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPp0w6guvBLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = client_openai.batches.retrieve('batch_682406de18708190bed4e66a925900b7')\n",
        "batch"
      ],
      "metadata": {
        "id": "LXYzLp0_3sWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567e0fdc-c510-4b62-e61a-454a12a451fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Batch(id='batch_682406de18708190bed4e66a925900b7', completion_window='24h', created_at=1747191518, endpoint='/v1/chat/completions', input_file_id='file-8BD39uTxNdwEURFFgnGjGF', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1747191783, error_file_id=None, errors=None, expired_at=None, expires_at=1747277918, failed_at=None, finalizing_at=1747191782, in_progress_at=1747191584, metadata={'description': 'Demo batch para video de YouTube'}, output_file_id='file-NuLMpRvBYwqoTjdyjtmMrc', request_counts=BatchRequestCounts(completed=5, failed=0, total=5))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.output_file_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RF9FFTZagyg4",
        "outputId": "e67de6b7-7044-4d80-bbd5-98d70f0403a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'file-NuLMpRvBYwqoTjdyjtmMrc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Recuperar los resultados\n"
      ],
      "metadata": {
        "id": "Hv5M5zH2skv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_results(batch: Dict[str,Any]) -> None:\n",
        "    \"\"\"\n",
        "    Descarga y procesa los resultados del batch completado.\n",
        "\n",
        "    Args:\n",
        "        batch: Objeto batch completado\n",
        "    \"\"\"\n",
        "    if not batch.output_file_id:\n",
        "        print('No hay archivo de salida disponible.')\n",
        "        return\n",
        "\n",
        "    print(f'Descargando resultados desde archivo: {batch.output_file_id}')\n",
        "\n",
        "    try:\n",
        "        # Descargar archivo de resultados\n",
        "        output_response = client_openai.files.content(batch.output_file_id)\n",
        "\n",
        "        # Guardar resultados en archivo local\n",
        "        output_path = './batch_results.jsonl'\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(output_response.content)\n",
        "\n",
        "        print(f'Resultados guardados en: {output_path}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error al descargar resultados: {e}')"
      ],
      "metadata": {
        "id": "_YsmxaDb4ZXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_results(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQF0QyF04x_N",
        "outputId": "3eda2fd6-a3d9-4364-f275-2a3da272fe45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando resultados desde archivo: file-NuLMpRvBYwqoTjdyjtmMrc\n",
            "Resultados guardados en: ./batch_results.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = '/content/batch_results.jsonl'\n",
        "\n",
        "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = [json.loads(line) for line in f]"
      ],
      "metadata": {
        "id": "_9b5deVRg8el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFHzy3JNsq7y",
        "outputId": "3bb98804-ed34-473f-94e8-733d2c1a1698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'batch_req_682407e697348190adbca2b9e4058d2b',\n",
              "  'custom_id': 'req1',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': 'fc9979efb290f7389be4c638414b1f89',\n",
              "   'body': {'id': 'chatcmpl-BWwffJXKnQSBDCLBQHkGCVV4rwTXx',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191727,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': '¡Arr, muchacho! La inteligencia artificial es como un loro bien entrenado, solo que en lugar de repetir palabras, este lorito metálico aprende y piensa por sí mismo. Se trata de un conjunto de tecnologías que permiten a las máquinas a pensar y resolver problemas como si fuesen humanos, utilizando datos y algoritmos para hacerse más listas y eficientes. Así que ten cuidado, si te descuidas, ¡podrían hasta apoderarse de la nave! Arrr 🏴\\u200d☠️.',\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 23,\n",
              "     'completion_tokens': 105,\n",
              "     'total_tokens': 128,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_d8864f8b6b'}},\n",
              "  'error': None},\n",
              " {'id': 'batch_req_682407e6a8408190967a3ade13a2e84f',\n",
              "  'custom_id': 'req2',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': 'e75092b0918919cb11bb4feffdd498a6',\n",
              "   'body': {'id': 'chatcmpl-BWwfhCk4p2rsYlzLZLk7gd4CmZdzr',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191729,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': \"¡Argh, compañero! Haré lo mejor pa' explicarlo en nuestro lenguaje pirata. Imagina te que estás navegando en altamar. El machine learning vendría a ser como un mapa de esos que encontramos en una isla desierta, un mapa que nos dice hacia dónde ir basándose en los tesoros que otros piratas ya han encontrado. Usa patrones previos pa' tomar decisiones y predecir adónde podrían esconderse los tesoros.\\n\\nAhora, el deep learning, ¡ah, es como tener un lorito sabio a tu lado que sabe de magia! Este lorito tiene una serie de capitanes pequeñitos en su cabeza, a los que llamamos capas, y cada uno toma decisiones basadas en lo que le dice el lorito anterior. Puede resolver acertijos más complejos y seguir pistas que son más difíciles pa' la mayoría de nosotros. A diferencia del mapa, que necesita que ya alguien haya pasado por ahí antes, el lorito puede aprender por sí mismo si ve suficientes pistas.\\n\\nEn resumen, el machine learning es como un mapa básico que necesita de un poco de guía humana, mientras que el deep learning es como un lorito mágico que aprende solo y puede ayudarte a encontrar tesoros escondidos más complicados. ¡Argh! ¡Espero que esta explicación sea tan clara como el agua del Caribe!\",\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 27,\n",
              "     'completion_tokens': 276,\n",
              "     'total_tokens': 303,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_d8864f8b6b'}},\n",
              "  'error': None},\n",
              " {'id': 'batch_req_682407e6ba508190b2dc348e7343e876',\n",
              "  'custom_id': 'req3',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': '22169deca0ecd9ae47089656b332ed9a',\n",
              "   'body': {'id': 'chatcmpl-BWwffqvJ7cPwltBiaigDDMqMS11UJ',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191727,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': '¡Ahoy, camarada del ciberespacio! El procesamiento de lenguaje natural, o PLN como dicen los sabios de la tecnología, tiene más usos que un cofre de tesoros en un galeón pirata. Aquí te cuento algunos, ¡argh!\\n\\n1. **Chatbots y asistentes virtuales** - Son como grumetes que entienden y responden a las preguntas del público, así como los loros repiten las palabras de un buen pirata.\\n\\n2. **Traducción automática** - Igual que un cartógrafo con mapas, esto convierte las palabras de un idioma a otro sin necesidad de atravesar océanos.\\n\\n3. **Análisis de sentimientos** - Para conocer si el viento sopla a favor o en contra en cuanto a las emociones de los clientes en las redes sociales o reseñas.\\n\\n4. **Reconocimiento de voz** - Como si oyera el eco de una voz de sirena, esta técnica convierte palabras habladas en texto.\\n\\n5. **Resumen de textos** - Estilo corsario, toma largas parrafadas y las resume, dejándolo tan claro como las aguas del Caribe.\\n\\n6. **Filtrado de spam** - Como un cazatesoros que detecta mensajes indeseados, manteniendo las bandejas libres de basura.\\n\\n7. **Extracción de información** - Como un bucanero experto en encontrar gemas escondidas entre montañas de texto.\\n\\n8. **Corrección de gramática** - Que revisen tus parrafadas más rápido que un cañonazo en pleno abordaje.\\n\\nEstos son solo algunos ejemplos, ¡pero las posibilidades son más vastas que el océano mismo! ¡Argh, y que los vientos de la innovación nos guíen! 🏴\\u200d☠️',\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 28,\n",
              "     'completion_tokens': 366,\n",
              "     'total_tokens': 394,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_90122d973c'}},\n",
              "  'error': None},\n",
              " {'id': 'batch_req_682407e6ca608190bec4cf7c2784ab6c',\n",
              "  'custom_id': 'req4',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': 'c5babe5bf7feeaa72cda3467cf12c5c1',\n",
              "   'body': {'id': 'chatcmpl-BWwffqsRraBtFGBaKBwXSSccf1KWW',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191727,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': '¡Argh, mi amigo! Permíteme contarte el relato sobre cómo funciona este tabernero virtual, ChatGPT. Este artilugio ingenioso es un modelo de lenguaje que ha sido entrenado con montones de datos, cual cofre del tesoro repleto de oro y mapas del conocimiento humano.\\n\\nPara empezar, cuando hacéis una pregunta, este bucanero de la inteligencia artificial la recibe, procesando cada palabra como si de un mapa del tesoro se tratara. Luego, escudriña sus archivos y patrones, buscando las gemas de la respuesta más adecuada para ti. Lo hace prediciendo las palabras que vendrán a continuación, basándose en su vasto conocimiento, de forma similar a cómo un viejo lobo de mar orienta su barco en la búsqueda de riquezas.\\n\\nY no creas que este corsario tiene cada respuesta grabada en piedra como un pergamino de un hechicero; más bien, genera las respuestas en el acto, siempre atento a los mares de información y ajustando el timón para ofrecer una respuesta propia y contextualizada, como si su brújula girara buscando el norte.\\n\\nAsí que, amigo, cada vez que conversas con este bot, estás navegando en un océano de información, ¡y su propósito es guiarte con sus palabras hasta la isla del entendimiento! ¡Arrr!',\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 24,\n",
              "     'completion_tokens': 285,\n",
              "     'total_tokens': 309,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_f5bdcc3276'}},\n",
              "  'error': None},\n",
              " {'id': 'batch_req_682407e6dd78819096d386fe46aee3e7',\n",
              "  'custom_id': 'req5',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': '1190f2d4fe04f13db0707d39c935ca5f',\n",
              "   'body': {'id': 'chatcmpl-BWwfghHEoiU12RdXaz5CDMheG9jJg',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191728,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': '¡Argh, grumete! Un modelo de embeddings es como el mapa del tesoro para las palabras en el mundo de la inteligencia artificial. En lugar de trabajar con las palabras como simples palabras, esos modelos las convierten en vectores, que vienen siendo como coordenadas en un vasto mar de posibilidades. Estos vectores capturan el significado y la relación entre las palabras, navegando por los mares de datos con precisión.\\n\\n¿Y para qué sirve, preguntas? ¡Para encontrar tesoros ocultos en los datos, claro está! Se utilizan en cosas como traducción de lenguajes, búsqueda de información, y procesamiento de lenguaje natural. Con estos modelos, podemos surcar las aguas del conocimiento, encontrando conexiones entre palabras que ni las sirenas habrían imaginado. ¡Así que a izar las velas y descubrir los secretos que esos embeddings tienen escondidos! ¡Argh! ☠️🏴\\u200d☠️',\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 28,\n",
              "     'completion_tokens': 193,\n",
              "     'total_tokens': 221,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_d8864f8b6b'}},\n",
              "  'error': None}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "responde_df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "jBEerlZThysq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responde_df['response'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5d6Kz5vh0mc",
        "outputId": "5bcabe17-bd96-448b-fe65-58d5de6b8045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status_code': 200,\n",
              " 'request_id': 'fc9979efb290f7389be4c638414b1f89',\n",
              " 'body': {'id': 'chatcmpl-BWwffJXKnQSBDCLBQHkGCVV4rwTXx',\n",
              "  'object': 'chat.completion',\n",
              "  'created': 1747191727,\n",
              "  'model': 'gpt-4o-2024-08-06',\n",
              "  'choices': [{'index': 0,\n",
              "    'message': {'role': 'assistant',\n",
              "     'content': '¡Arr, muchacho! La inteligencia artificial es como un loro bien entrenado, solo que en lugar de repetir palabras, este lorito metálico aprende y piensa por sí mismo. Se trata de un conjunto de tecnologías que permiten a las máquinas a pensar y resolver problemas como si fuesen humanos, utilizando datos y algoritmos para hacerse más listas y eficientes. Así que ten cuidado, si te descuidas, ¡podrían hasta apoderarse de la nave! Arrr 🏴\\u200d☠️.',\n",
              "     'refusal': None,\n",
              "     'annotations': []},\n",
              "    'logprobs': None,\n",
              "    'finish_reason': 'stop'}],\n",
              "  'usage': {'prompt_tokens': 23,\n",
              "   'completion_tokens': 105,\n",
              "   'total_tokens': 128,\n",
              "   'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "   'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "    'audio_tokens': 0,\n",
              "    'accepted_prediction_tokens': 0,\n",
              "    'rejected_prediction_tokens': 0}},\n",
              "  'service_tier': 'default',\n",
              "  'system_fingerprint': 'fp_d8864f8b6b'}}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}